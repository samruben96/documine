<?xml version="1.0" encoding="UTF-8"?>
<story-context story="11.1" title="Async Processing Architecture" created="2025-12-04">
  <summary>
    Transform document processing from synchronous HTTP trigger to asynchronous pg_cron-based
    job queue to eliminate 504 timeout errors on documents requiring 100-250+ seconds processing.
  </summary>

  <problem>
    <current-state>
      - Upload calls createProcessingJob() which triggers Edge Function synchronously via HTTP
      - Edge Function HTTP trigger times out at ~150s even though function has 480s execution limit
      - Post-Epic 10, processing takes 100-250+ seconds (Docling + chunking + embeddings + extraction)
      - Result: 504 Gateway Timeout errors for complex documents
    </current-state>
    <evidence>
      Supabase Edge Function logs show:
      - POST /process-document 504 150.175s (failed)
      - POST /process-document 200 121.416s (barely succeeded)
    </evidence>
  </problem>

  <solution>
    <architecture>
      Upload → Create processing_job → Return immediately (< 2s)
                     ↓
      pg_cron (every 10s) → Pick pending job → Invoke Edge Function via pg_net
                     ↓
      Edge Function processes → Updates progress_data via Realtime
                     ↓
      UI subscribes to Realtime → Shows live progress
    </architecture>
  </solution>

  <acceptance-criteria>
    <ac id="11.1.1" title="Processing Jobs Table">
      - Add agency_id column (FK to agencies)
      - Add stage column (varchar(30) default 'queued')
      - Add progress_percent column (integer 0-100)
      - Add retry_count column (integer default 0)
      - Enable Realtime on processing_jobs
      - RLS policies for agency isolation
    </ac>
    <ac id="11.1.2" title="Upload Flow Change">
      - Document upload creates processing_job record with status='pending'
      - Upload returns immediately (< 2 seconds)
      - NO synchronous Edge Function trigger
    </ac>
    <ac id="11.1.3" title="pg_cron Job Processor">
      - Enable pg_cron and pg_net extensions
      - Create process_next_document_job() function
      - Use FOR UPDATE SKIP LOCKED to prevent race conditions
      - Schedule cron job every 10 seconds
      - Invoke Edge Function via pg_net.http_post()
    </ac>
    <ac id="11.1.4" title="Edge Function Modification">
      - Accept job_id parameter
      - Update processing_jobs.progress_percent during processing
      - Update processing_jobs.stage at each stage transition
      - Mark job completed/failed on finish
    </ac>
    <ac id="11.1.5" title="Realtime Updates">
      - processing_jobs changes broadcast via Supabase Realtime
      - UI receives updates within 500ms
    </ac>
    <ac id="11.1.6" title="Processing Success">
      - Documents up to 300s processing time complete successfully
      - No 504 errors on upload
      - Processing continues even if user navigates away
    </ac>
  </acceptance-criteria>

  <existing-code>
    <file path="supabase/functions/process-document/index.ts">
      - Already has progress_data updates via updateJobProgress()
      - Already has stage constants (STAGE_WEIGHTS, STAGE_DISPLAY_NAMES)
      - Needs: Accept job_id param, update stage/progress_percent columns
    </file>
    <file path="src/lib/documents/service.ts">
      - createProcessingJob() creates job and triggers Edge Function synchronously
      - Needs: Remove triggerEdgeFunction() call
    </file>
    <file path="src/hooks/use-processing-progress.ts">
      - Already subscribes to Realtime for processing_jobs.progress_data
      - Should work as-is with new async flow
    </file>
  </existing-code>

  <database-changes>
    <migration name="add_async_processing_infrastructure">
      -- Step 1: Enable extensions
      CREATE EXTENSION IF NOT EXISTS pg_cron WITH SCHEMA extensions;
      CREATE EXTENSION IF NOT EXISTS pg_net WITH SCHEMA extensions;

      -- Step 2: Add missing columns to processing_jobs
      ALTER TABLE processing_jobs
        ADD COLUMN IF NOT EXISTS agency_id uuid REFERENCES agencies(id) ON DELETE CASCADE,
        ADD COLUMN IF NOT EXISTS stage varchar(30) DEFAULT 'queued',
        ADD COLUMN IF NOT EXISTS progress_percent integer DEFAULT 0,
        ADD COLUMN IF NOT EXISTS retry_count integer DEFAULT 0;

      -- Step 3: Add constraint and index
      ALTER TABLE processing_jobs
        ADD CONSTRAINT check_progress_percent CHECK (progress_percent BETWEEN 0 AND 100);

      CREATE INDEX IF NOT EXISTS idx_processing_jobs_pending
        ON processing_jobs(created_at)
        WHERE status = 'pending';

      -- Step 4: Enable Realtime (likely already enabled)
      ALTER PUBLICATION supabase_realtime ADD TABLE processing_jobs;

      -- Step 5: Create job processor function
      CREATE OR REPLACE FUNCTION process_next_document_job()
      RETURNS void
      LANGUAGE plpgsql
      SECURITY DEFINER
      SET search_path = public, extensions
      AS $$
      DECLARE
        next_job processing_jobs%ROWTYPE;
        v_storage_path text;
        v_supabase_url text;
        v_service_key text;
      BEGIN
        -- Atomic job pickup with SKIP LOCKED
        SELECT * INTO next_job
        FROM processing_jobs
        WHERE status = 'pending'
        ORDER BY created_at ASC
        LIMIT 1
        FOR UPDATE SKIP LOCKED;

        IF next_job.id IS NULL THEN
          RETURN;
        END IF;

        -- Mark as processing
        UPDATE processing_jobs
        SET status = 'processing',
            stage = 'downloading',
            started_at = now()
        WHERE id = next_job.id;

        -- Get document storage path
        SELECT storage_path INTO v_storage_path
        FROM documents
        WHERE id = next_job.document_id;

        -- Get Supabase URL and service key from settings
        v_supabase_url := current_setting('supabase.project_url', true);
        v_service_key := current_setting('supabase.service_role_key', true);

        -- Invoke Edge Function via pg_net
        PERFORM net.http_post(
          url := v_supabase_url || '/functions/v1/process-document',
          headers := jsonb_build_object(
            'Authorization', 'Bearer ' || v_service_key,
            'Content-Type', 'application/json'
          ),
          body := jsonb_build_object(
            'job_id', next_job.id::text,
            'document_id', next_job.document_id::text,
            'storage_path', v_storage_path,
            'agency_id', next_job.agency_id::text
          )
        );
      END;
      $$;

      -- Step 6: Schedule cron job (runs every 10 seconds)
      SELECT cron.schedule(
        'process-documents',
        '*/10 * * * *',  -- Every minute (pg_cron minimum)
        'SELECT process_next_document_job()'
      );
    </migration>
  </database-changes>

  <service-changes>
    <file path="src/lib/documents/service.ts">
      Change createProcessingJob() to:
      1. Include agency_id when inserting processing_job
      2. Remove triggerEdgeFunction() call entirely
      3. Return immediately after creating the job record
    </file>
  </service-changes>

  <edge-function-changes>
    <file path="supabase/functions/process-document/index.ts">
      1. Accept job_id in request body
      2. Update processing_jobs.stage and progress_percent columns directly
      3. Keep existing progress_data updates for backward compatibility
    </file>
  </edge-function-changes>

  <constraints>
    <constraint type="pg_cron">
      pg_cron minimum interval is 1 minute. For 10-second polling, we need to use
      alternative approach: schedule every minute but process multiple jobs per run,
      OR use Supabase's job scheduling feature if available.
    </constraint>
    <constraint type="pg_net">
      pg_net HTTP calls are asynchronous - function returns immediately.
      Need to handle case where Edge Function fails after pg_net call.
    </constraint>
  </constraints>

  <testing>
    - Upload document and verify returns in < 2 seconds
    - Verify job picked up within 60 seconds (pg_cron minimum)
    - Verify processing completes for 200+ second documents
    - Verify Realtime updates flow to UI
    - Verify failed jobs have correct status and error_message
  </testing>

  <references>
    - Supabase pg_cron: https://supabase.com/docs/guides/database/extensions/pg_cron
    - Supabase pg_net: https://supabase.com/docs/guides/database/extensions/pg_net
    - PostgreSQL SKIP LOCKED: https://www.postgresql.org/docs/current/sql-select.html
  </references>
</story-context>
